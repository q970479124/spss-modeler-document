## 一、数据准备阶段 （3-5天） ##
### 本阶段目标： ###
在明确业务目标的基础上，完成目标数据的提取、清洗，为后续建模提供高质量输入数据   

1. **明确业务目标与问题定义：**
   > **说明：** 在数据准备之前，需要明确预测的业务场景、目标变量、样本对象及指标要求，以保证后续建模方向正确、可衡量  
   > **目的：** 确保数据准备阶段的工作围绕清晰的业务问题展开，避免后期重复调整或数据偏离

   - **确认预测目标：** 确定模型要预测的目标变量为 `素子数`
   - **确认预测业务对象：** 限定为电容型号 `03ENA` 和 `15XAW_GP`
   - **确认数据时间范围与量级：** 确认两个型号的数据是否覆盖最近 `3` 年，且样本量足够进行建模
   - **确认当前人工可达指标：** 例如当前人工预测 `ΔC` 的误差控制在 `2%` 以内
   - **设定预期目标指标：** 例如希望通过建模将 `ΔC` 误差降低 `20%`

2. **数据收集与整合：**
   > **说明：** 从客户数据库中获取目标型号的历史数据，并在 CP4D 平台中进行整合、可视化与初步分析，为后续清洗与特征工程提供基础   
   > **目的：** 确保所用数据一致、结构清晰，为后续清洗与特征工程提供基础

   - 使用 CP4D → 连接数据源节点 连接客户数据库
   - 在 Data Refinery 中编写 SQL，分别获取型号 03ENA 与 15XAW_GP 的数据
   - 利用 Data Refinery 可视化功能 生成探索性数据分析（EDA）报告，查看数据分布与质量
   - 在 SPSS Modeler 中使用「导入 → 数据资产节点」导入生成的数据资产
   - 使用「字段操作 → 类型节点」自动识别字段类型
   - 使用「输出 → 数据审计节点」生成并查看初步数据质量报告

3. **数据清洗（缺失值、异常值、重复值处理）：** 
   > **说明：** 对原始数据进行初步`（简单）`清洗，包括字段筛选、异常检测、缺失补全和重复值处理，确保数据质量基本满足建模需求  
   > **目的：** 提高数据完整性与一致性，剔除异常与噪声，保证输入数据的可靠性与可解释性

   - **质量审查：** 通过数据审计报告或 EDA 报告识别缺失值、异常值、重复记录及字段问题
   - **字段筛选：** 结合业务知识，使用「字段操作 → 过滤节点」去除无关或冗余字段
   - **缺失与异常处理：** 使用「字段操作 → 填充、派生、过滤节点」进行缺失值填补和异常值替换或删除
   - **重复与异常记录处理：** 使用「记录操作 → 选择、排序、总汇、合并节点」清除重复或异常记录
   - **结果验证：** 使用「输出 → 表 / 数据审计节点」生成清洗后数据质量报告，验证清洗效果

4. **特征命名规范化：**
   > **说明：** 统一数据字段命名规则，确保各字段在建模与结果解释中具有一致性与可读性  
   > **目的：** 提升数据可维护性与共享性，为特征工程、建模与后期可视化提供清晰结构  

   - 检查字段名称是否包含特殊符号或不一致命名
   - 按照统一规范（如英文字母+下划线命名）进行重命名
   - 若存在同义字段或业务重叠字段，进行合并与命名统一
   - 在字段属性中保留业务含义说明，方便后续追踪。

<br>
**阶段产出成果**  

| 分类     | 产出内容                                       |
| ------ | ------------------------------------------ |
| 明确目标   | 预测目标说明文档（素子数预测目标、指标定义、预期目标）                |
| 数据获取   | CP4D 数据连接与抽取 SQL 脚本、03ENA & 15XAW_GP 原始数据集 |
| 数据报告   | Data Refinery EDA 报告、SPSS 数据审计报告           |
| 数据清洗结果 | 清洗后高质量数据资产（含缺失/异常/重复处理）                    |
| 规范文档   | 字段命名规范及最终字段说明文档                            |

<br>

## 二、数据探索与理解 （7-10天） ##
### 本阶段目标： ###
系统地分析、可视化与理解清洗后的数据结构、分布与变量关系，识别潜在模式、异常值及关键特征，为后续建模和特征工程提供依据  
1. **描述性统计分析：**
   > **说明：** 对数据集进行基础统计分析，了解每个变量的总体特征，包括缺失情况、分布范围、集中趋势与离散程度  
   > **目的：** 掌握数据的总体概貌与分布规律，初步判断数据质量和变量可用性

   - 通过分析`探索性数据分析报告`或`数据审计报告`确定数据质量（例如缺失情况、最大/最小值、标准差、熵等信息）
   - 通过描述性统计确认数据是否存在极端值或明显错误
   - 初步评估后续数据清洗与特征工程的重点字段

2. **可视化探索（分布图、箱线图等）：**
   > **说明：** 利用可视化手段直观展示数据分布形态、偏态程度与异常点  
   > **目的：** 通过图形化展示快速识别数据规律、异常模式与分布特征，为特征工程提供直觉判断

   - **使用`探索性数据分析报告`的可视化功能查看：**
     - `直方图（Histogram）` 与 `Q–Q图（Quantile–Quantile Plot）`，判断是否服从正态分布
     - `箱线图（Boxplot）`，识别异常值分布及中位数位置
   - **重点检查：**
     - 分布是否偏斜
     - 是否存在极端离群值
     - 样本量是否平衡

3. **变量间相关性分析：**
   > **说明：** 通过统计与可视化手段分析变量之间的相关关系，识别潜在的共线性或依赖关系  
   > **目的：** 明确不同变量间的相关性结构，为后续特征筛选与模型输入变量选择提供依据

   - 在`探索性数据分析(EDA)报告` → 数据质量模块中查看：
     - `皮尔逊相关系数`、`斯皮尔曼相关系数`
     - 高相关（|r| > 0.8）的字段是否存在共线性
   - 使用可视化手段:
     - `散点图（Scatter Plot）` 分析变量关系
     - `热力图（Correlation Heatmap）` 查看整体相关矩阵
     - 观察异常点与峰值分布趋势

4. **异常点与离群值检测：**
   > **说明：** 对异常值、离群点及极端值进行系统检测与标注，评估其成因与对模型的影响  
   > **目的：** 识别并合理处理不符合总体规律的样本，避免建模时造成误导或过拟合  
   
   - 结合前几步的分布分析与箱线图结果，确定：
     - 异常值区间
     - 极值上下界
     - 离群点比例与位置
   - 根据业务与统计标准：
     - 删除明显错误记录
     - 或通过均值/中位数/插值法修正异常点
   - 在 SPSS Modeler 中可使用：
     - `数据审计节点`查看异常分布
     - `过滤节点`筛除异常样本
     - `派生节点`对异常值进行替换或标记

5. **变量重要性初步筛查：**
   > **说明：** 利用树模型（如随机森林、CHAID 或 C&R Tree）快速计算变量的重要性排序  
   > **目的：** 识别对目标变量（素子数）影响最大的特征字段，为后续特征工程和建模提供优先方向  
   
   - 使用 SPSS Modeler 的随机森林节点（或自动建模节点）进行初步预测
   - 通过`模型输出 → 特征重要性`报告获取各字段的重要性得分
   - 对得分较低、噪声高或冗余字段进行初步筛除
   - 输出特征重要性列表，作为建模阶段输入字段选择参考
  
**阶段产出成果**

| 分类      | 产出内容                   |
| ------- | ---------------------- |
| 描述性统计   | 各字段统计特征报告（均值、方差、缺失率、熵） |
| 可视化结果   | 分布图、Q–Q图、箱线图、散点图、热力图   |
| 变量相关性分析 | 相关矩阵与共线性报告             |
| 异常与离群检测 | 异常值检测报告及修正策略           |
| 变量重要性   | 初步特征重要性排序表             |


<br>

## 三、特征工程 （7-10天） ##
1. 特征选择（过滤法、包裹法、嵌入法）
   - 首先采用过滤法：通过Pearson 相关系数初步筛选出高相关性特征，以及对极高相关性特征的删除和合并
   - 采用嵌入法：通过模型初步跑出数据重要性，保留高重要性数据和组合特征(参考`变量重要性初步筛查`)
   - 采用包裹法：使用模型进行递归验证和训练，不断追加或删除特征验证模型效果，多轮训练后总结出效果最佳的特征

2. 特征构造（组合特征、统计特征等）
   - 通过以上分析以及业务的分析构建新特征

3. 特征编码（One-Hot、Label Encoding 等）
   - 对字符型或有规律、有业务分类的连续型/分类型数据进行独热编码或标签编码
   - 注：树模型可以不做转换

4. 特征缩放（标准化、归一化等）
   - 使用spss modeler 字段操作 - 派生/填充/重新分类节点进行标准化和归一化转换
   - 注：树模型可以不做转换

5. 降维处理（PCA、LDA、ICA 等）
   - 使用spss modeler 建模 - PCA节点进行特征降维处理

6. 特征重要性评估与优化
   - 使用树模型重新验证特征重要性评估
   - 以上1-6步骤不按顺序，交叉多轮处理，选择出效果最好的特征组合

<br>

## 四、建模准备 （5-7天） ##
1. 数据划分（训练集、验证集、测试集）
   - 使用spss modeler字段操作 - 分区节点将数据分为训练集和测试集(一般为7-3或8-2)
   - 使用spss modeler建模 - 自动分类器节点进行交叉验证

2. 模型选择
   - 回归类模型选择：采用多个模型进行多组训练和验证，筛选出效果最好的一组模型(按照对数据状况的分析优先采用梯度提升模型确认模型精准度是否达标)

3. 定义评价指标
   - R²
   - RMSE
   - MAPE
   - ΔC
   - 通过上述指标确认模型精度，R²和RMSE作为模型训练时的评判标准，MAPE和ΔC作为测试和验证的评价指标
   - 通过spss modeler 输出-分析节点和生成的模型结果查看预测结果分布状况

<br>

## 五、模型训练与评估 （5-7天） ##
1. 基础模型训练
   - 采用人工预测特征进行模型训练
   - 采用以上筛选出的特征进行模型训练

2. 参数调优
   - 树模型使用时采用超参数优化调整模型训练效果

3. 模型性能对比与筛选
   - 筛选出综合效果最佳的模型

4. 使用测试集/验证集评估性能
   - 通过spss modeler生成的模型查看模型结果
   - RMSE / MAPE / R² / ΔC 等指标计算
   - 误差分析：查看正态分布状况分析导致误差的原因
   - 多轮验证模型稳定性和鲁棒性(可能放在陪跑的模型运行阶段)

<br>

## 六、模型部署与监控 （3-5天） ##
1. 模型导出与上线（API、批处理、嵌入系统）
   - 在spss modeler 模型中导出模型并部署在CP4D中
   - 构建API及可视化画面调用部署后的模型

2. 性能监控（漂移检测、精度监控）
   - 模型上线运行阶段持续检测模型性能和精度
