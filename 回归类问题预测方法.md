# 回归预测方法 #
## 一、 数据探索 ##
`模型看到是结果而不是原始值`  

### 1. 直方图 ###
> 直方图本质是展示连续数据分布规律的图表，X 轴代表数据的分组区间，Y 轴代表对应区间的数据频次。
<img width="1375" height="907" alt="image" src="https://github.com/user-attachments/assets/fe0779bd-c41c-4ba7-89be-433f914aed52" />

<br><br>
> - X轴为数据分组，如房价在十万、二十万、三十万...的连续分组。
> - Y轴为X轴分组范围内出现的频率范围。

> **说明：** 直方图主要能看出数据分布状态，比如正态分布、偏态分布
>   偏态分布又分为左偏态(负偏)和右偏态(正偏)
>   - 左偏态： 偏度值 <0，分布的 “长尾” 在左侧，均值 < 中位数 < 众数。
>   - 右偏态： 偏度值 > 0，分布的 “长尾” 在右侧，均值 > 中位数 > 众数。
>   - 对称分布： 偏度值 ≈ 0，均值 ≈ 中位数 ≈ 众数，如正态分布。

> **直方图能看出的内容**
> 1. 数据分布
> 2. 异常值
> 3. 数据集中
> 4. 分箱方式：将 “1 个连续字段” 转换为 “1 个分类字段”（或 “多个哑变量字段”）

**树模型时直方图的作用**
| 目的                  | 为什么重要                                            |
| ------------------- | ------------------------------------------------ |
| 1️⃣ 检查异常值（Outliers） | 偏态可能掩盖极端点，树会错误地以异常值分裂                            |
| 2️⃣ 判断是否需要分箱或截断     | 如果极端长尾太长，模型学习效率会变差                               |
| 3️⃣ 数据理解            | 了解特征分布能帮助你解释模型输出与特征重要性                           |
| 4️⃣ 调参辅助            | 极端偏态时，`max_depth`、`min_child_weight`等参数的最佳值可能会变化 |
| 5️⃣ 提升可解释性          | 对数变换后，特征变化和结果关系更直观                               |

---

#### 处理偏态 ####
> 右偏态可以直接处理，左偏态需要转换成右偏态再处理（如果模型敏感或需要解释性，就需要转换回来）
> 模型看到是结果而不是原始值

**偏态处理流程**
| 步骤     | 操作                                 | 工具/方法                              |
| ------ | ---------------------------------- | ---------------------------------- |
| ① 判断   | 看直方图 + 箱线图 + 计算偏度                  | `sns.histplot`, `scipy.stats.skew` |
| ② 判断影响 | 判断目标/特征、模型类型                       | 表格决策                               |
| ③ 变换   | log / sqrt / Box-Cox / Yeo–Johnson | `np.log1p`, `PowerTransformer`     |
| ④ 验证   | 重新画图 + 比较偏度 + 比较模型RMSE             | `skew()`, `mean_squared_error`     |
| ⑤ 落地   | 在建模/预测中使用变换后的值                     | `exp()` 反变换预测值                     |

---

**偏度计算**
>  - 皮尔逊偏度系数（Pearson Skewness）
>  - 公式为：\(Sk_1 = \frac{3(\bar{x} - Med)}{s}\)
>    - \(\bar{x}\)（样本均值）：所有数据的平均值，对极端值敏感，是偏度计算的 “核心参照”。
>    - Med（样本中位数）：将数据从小到大排序后，位于中间位置的数值，对极端值不敏感，是衡量分布 “中心位置” 的稳定指标。
>    - s（样本标准差）：衡量数据整体离散程度的指标，在这里作为 “标准化因子”，让偏度系数不受数据单位影响（比如身高用厘米或米，偏度结果一致）。
>    - 系数 “3”：是经验系数，通过大量数据验证，用 3 倍的 “均值 - 中位数” 差值除以标准差，能让偏度结果更贴合实际分布的偏斜感知。

|      | 偏度绝对值   | 含义   | 处理建议     |
| ---- | ------- | ---- | -------- |
| 轻微偏态 | < 0.5   | 近似对称 | 可不处理     |
| 中度偏态 | 0.5～1.0 | 偏态明显 | 考虑轻度变换   |
| 严重偏态 | > 1.0   | 强偏态  | 建议变换或标准化 |

---

**判断是否需要处理偏态**
| 情况                       | 是否需要处理              |       |        |
| ------------------------ | ------------------- | ----- | ------ |
| 目标变量用于线性回归、线性建模          | ✅ 需要（保持残差正态）        |       |        |
| 特征变量用于线性或距离模型            | ✅ 需要（保持尺度合理）        |       |        |
| 树模型（RF/XGBoost/LightGBM） | ❌ 可不处理              |       |        |
| 偏态不大（                    | skew                | <0.5） | ❌ 可不处理 |
| 数据带 0 或负值                | ⚠️ 不可直接取 log，需要特殊变换 |       |        |

---

**偏态处理方法**
| 方法                 | 适用条件      | Python 实现                                                      | 说明          |
| ------------------ | --------- | -------------------------------------------------------------- | ----------- |
| **log-transform**  | 数据>0 且右偏  | `np.log(y)` 或 `np.log1p(y)`                                    | 压缩右尾        |
| **平方根变换 (sqrt)**   | 数据≥0，轻度右偏 | `np.sqrt(y)`                                                   | 较温和         |
| **Box–Cox 变换**     | 数据>0      | `scipy.stats.boxcox(y)`                                        | 自动找最佳λ      |
| **Yeo–Johnson 变换** | 数据可含负值    | `sklearn.preprocessing.PowerTransformer(method='yeo-johnson')` | 广义适用        |
| **反转 + log**       | 左偏（负偏）    | `np.log(y.max()+1 - y)`                                        | 把左偏变右偏再 log |

---

**常用处理**
| 方法                        | 适用场景    | 说明         |
| ------------------------- | ------- | ---------- |
| 对数/Box–Cox/Yeo–Johnson 变换 | 特征为正且右偏 | 平滑分布、线性化   |
| 分箱（binning）或离散化           | 极度偏态或长尾 | 用分位数切分更稳健  |
| Winsorization（截尾）         | 有极端离群值  | 限制极端值对模型影响 |
| 标准化或归一化                   | 轻微偏态    | 缩放到统一范围    |

---

**进阶：偏态后的建模策略**
| 模型类型          | 偏态处理重点              | 备注          |
| ------------- | ------------------- | ----------- |
| 线性回归          | ✅ 对目标变量做 log        | 确保残差正态      |
| Lasso / Ridge | ✅ 对所有偏态特征做 log/sqrt | 提升收敛与解释性    |
| 树模型           | ⚙️ 可选               | 有时可只平滑极端值   |
| 时间序列 / 回归预测   | ✅ 对目标 log，预测后反变换    | 常用于预测销量、价格等 |
| 聚类 / PCA      | ✅ 对特征变换             | 减少方差主导效应    |

---

<br><br><br>
## 2. Q-Q图(分位图) ##
<img width="295" height="244" alt="image" src="https://github.com/user-attachments/assets/0ffe75b8-ee13-4dc1-8ab5-7297b162981b" />

<img width="314" height="243" alt="image" src="https://github.com/user-attachments/assets/611e1954-50bc-402d-bfcb-c3c1b13ecfe8" />

> Q-Q 图本质是通过对比数据的分位数与理论分布的分位数，来检验数据分布一致性的图形，x 轴代表理论分位数，y 轴代表实际数据分位数。
> X轴：理论分布的分位数，`根据 “正态分布” 算出的 “标准答案”`
> Y轴：实际数据的经验分位数，`收集的真实数据中排序后得到的 “实际值”`  

>Q-Q 图是“样本分布与理论分布”的分位数对比图。
>若点落在直线上 → 数据服从理论分布；
>若尾部偏离 → 说明存在偏态或重尾现象。
<img width="1048" height="294" alt="image" src="https://github.com/user-attachments/assets/2a88b35e-32bc-4a83-94f9-47c16e46e0cd" />

**上图展示了三种典型 Q-Q 图：**
> - 左图（正态分布）：点几乎落在直线上 → 数据服从正态分布。
> - 中图（右偏分布）：右上角的点向上翘 → 尾部更长 → 右偏。
> - 右图（左偏分布）：左下角的点向下弯 → 左尾更长 → 左偏。
> 通过观察尾部是否偏离直线，就能快速判断数据偏态方向和是否接近正态，非常适合用于判断是否需要对数变换或其他数据标准化处理。

`单个弯或直线说明了数据属于偏态分布或理论分布，但是出现多个弯折时说明数据需要分组预测，每个弯服从的分布规律不一致。`

**尾处理**
| 现象               | 含义                         | 说明               |
| ---------------- | -------------------------- | ---------------- |
| **右上角点偏离直线、往上翘** | **右尾太厚**（heavy right tail） | 极大值太多 / 太大       |
| **右上角点往下弯**      | **右尾太轻**（light right tail） | 极大值太少 / 太集中      |
| **左下角点偏离直线、往下翘** | **左尾太厚**（heavy left tail）  | 极小值太多 / 太小       |
| **左下角点往上弯**      | **左尾太轻**（light left tail）  | 极小值太少 / 太集中      |
| **两端都翘**         | **双尾厚尾**（heavy tails）      | 比正态分布更极端（如 t 分布） |

**为什么要做“尾处理（Tail Treatment）**
> 因为 Q–Q 图的尾部偏离说明：
> 数据中存在极端值、异常值或非正态尾特征；
> 极端尾会显著影响模型学习；
> 特别是对 线性模型、统计检验、残差正态性假设 非常致命。
>
> 举例：
> 在回归模型中，如果右尾很厚（极大值多），
> 残差分布就会右偏 → 置信区间、p 值全失真。

`处理方式和直方图一致`

<br><br><br>
### 3. 箱线图 ###
> 箱线图（Box Plot，又称箱须图）是一种通过五数概括法直观展示数据分布特征的统计图，核心价值是快速识别数据的集中趋势、离散程度和异常值，尤其适合多组数据的对比分析。
<img width="1552" height="1214" alt="image" src="https://github.com/user-attachments/assets/caf018c3-1bd0-4a20-abe9-78505fe5ba3e" />

**五数概括的具体含义**
| 统计量	             | 计算方式                                                           | 作用                        |
| -------------------- | ----------------------------------------------------------------- | --------------------------- |
| 最小值（Min）	       | 原始数据中除异常值外的最小观测值（非绝对最小值，需排除极端异常值）      | 反映数据的下限范围            |
| 第一四分位数（Q1）	   | 将数据从小到大排序后，位于 25% 位置的数值（即有 25% 的数据小于该值）	 | 反映数据的 “下四分位水平”     |
| 中位数（Q2/Median）	 | 将数据从小到大排序后，位于 50% 位置的数值（即有 50% 的数据小于该值）	 | 反映数据的集中趋势（抗极端值） |
| 第三四分位数（Q3）	   | 将数据从小到大排序后，位于 75% 位置的数值（即有 75% 的数据小于该值）	 | 反映数据的 “上四分位水平”     |
| 最大值（Max）	       | 原始数据中除异常值外的最大观测值（非绝对最大值，需排除极端异常值）	     | 反映数据的上限范围            |

> - 四分位距（IQR）：IQR = Q3 - Q1，代表数据中间 50%（从 Q1 到 Q3）的离散程度，IQR 越大，说明数据中间部分的波动越剧烈。
> - 异常值（Outlier）：通过 IQR 设定判断标准 ——
>    低于「Q1 - 1.5×IQR」或高于「Q3 + 1.5×IQR」的数值，称为轻度异常值（通常用 “圆点” 标记）；
>    低于「Q1 - 3×IQR」或高于「Q3 + 3×IQR」的数值，称为极端异常值（通常用 “星号” 标记）。

> X轴：分类条件
> Y轴：连续的分区数值

**箱线图的观测要点（关键解读方法）**  
> 观测箱线图时，需重点关注 5 个维度，避免只看 “高低”：
> 1. 看中位数：判断集中趋势
> 中位数的位置高低：直接反映该组数据的 “中间水平”，如班级 2 中位数 78 分，说明班级 2 有 50% 的学生成绩≥78 分，整体水平高于其他班级。
> 中位数在箱子中的位置：若中位数在箱子正中间，说明数据接近对称分布；若靠近 Q1（左偏）或 Q3（右偏），说明数据存在偏态。
> 2. 看箱子宽度：判断离散程度
> 箱子的宽度即 IQR，代表数据中间 50% 的波动范围：
> 箱子越宽，IQR 越大，说明数据中间部分的离散程度越高（成绩差距越大）；
> 箱子越窄，IQR 越小，说明数据中间部分越集中（成绩越平均）。
> 3. 看箱须长度：判断尾部离散
> 箱须的长度反映 “非异常值的最值” 与箱子的距离：
> 上须越长，说明数据的 “上尾”（大值）越分散（如右偏数据的上须长）；
> 下须越长，说明数据的 “下尾”（小值）越分散（如左偏数据的下须长）；
> 若某一方向的箱须极短（甚至与箱子重合），说明该方向的数值高度集中（如多数学生成绩接近 Q3，下须短）。
> 4. 看异常值：判断极端情况
> 异常值的数量：异常值越多，说明数据越可能存在极端情况（如班级有个别学生缺考得 0 分，或有学霸得满分）；
> 异常值的位置：异常值远离箱须，说明是极端异常（如成绩 10 分，远低于 Min=55），需优先排查是否为数据错误；异常值靠近箱须，说明是轻度异常（如成绩 95 分，略高于 Max=90），可能是真实的高成绩。
> 5. 多组对比：看整体差异
> 横向对比不同组的箱线图时，重点看 “中位数位置差”“箱子重叠度”：
> 若两组箱子几乎无重叠（如班级 2 的 Q1=75，班级 3 的 Q3=70），说明两组数据差异显著（班级 2 整体成绩高于班级 3）；
> 若两组箱子大量重叠（如班级 1 的 Q3=80，班级 2 的 Q1=75），说明两组数据差异较小，需结合其他指标（如均值）进一步判断。

**箱须的计算**
> 第一步：计算关键统计量
> 排序数据：将原始数据从小到大排列。
> 找 Q₁和 Q₃：
> Q₁（第一四分位数）：数据排序后，位于 25% 位置的数值（即 25% 的数据比它小）。
> Q₃（第三四分位数）：数据排序后，位于 75% 位置的数值（即 75% 的数据比它小）。
> 计算 IQR：IQR = Q₃ - Q₁（反映中间 50% 数据的离散程度）。
> 第二步：确定异常值范围
> 异常值的判断阈值是：
> 下限 = Q₁ - 1.5×IQR
> 上限 = Q₃ + 1.5×IQR
> 小于下限或大于上限的数值，被判定为异常值。
> 第三步：找非异常的最值
> 从原始数据中排除所有异常值，剩下的数里：
> 非异常最大值 = 剩余数据中的最大值
> 非异常最小值 = 剩余数据中的最小值

<br><br><br>
### 4. 散点图 ###
> 散点图是一种通过点的分布形态来展示两个连续变量之间关系的可视化工具，核心作用是帮助我们快速识别变量间的相关性、趋势和异常模式。
<img width="771" height="600" alt="image" src="https://github.com/user-attachments/assets/df0597f2-75d4-4656-9422-2c0ed627962c" />

> X轴 Y轴 分别为两个连续型变量

> 1. 判断 “相关性方向”
> 正相关：点从左下到右上呈趋势分布（如 “广告投入越多，销售额越高”）。
> 负相关：点从左上到右下呈趋势分布（如 “价格越高，销量越低”）。
> 无相关：点随机分布，无明显趋势（如 “用户年龄与喜欢的颜色”）。
> 2. 识别 “关系强度”
> 强相关：点紧密聚集在一条直线附近（如 “身高与体重”）。
> 弱相关：点呈趋势但较分散（如 “学习时间与考试分数”）。
> 3. 发现 “异常点”
> 远离整体趋势的点（如 “广告投入很高但销售额极低”），可能是数据错误或特殊案例，需单独分析。
> 4. 捕捉 “非线性关系”
> 若点呈 “曲线、弧形” 分布，说明变量间是非线性关系（如 “用户年龄与 APP 使用时长” 可能先升后降）。

`例如下图500000位置为上限值，可以做新特征生成，1=上限 0=上限以外`  
<br>
<img width="1720" height="1232" alt="image" src="https://github.com/user-attachments/assets/756384a9-c049-4471-9f7d-0c1eddeee3ec" />

<br><br><br>
## 二、 特征工程 ##
`特征工程的核心是：筛选有效特征、创造新特征、验证其效果。`
`筛选 - 创造 -验证`
1. 筛选有效特征的基本方法
   - 目的：减少噪声，突出重要因素
   - 相关性分析： 一般采用皮尔逊相关性分析，需要注意的是相关性高或低并不意味着该特征是无效特征。
     - 相关性（Correlation）通常衡量的是线性关系。但真实世界里的变量关系往往是非线性的、复杂交互的。
     - 低相关性的特征，组合起来可能呈现一个高相关性
     - 伪相关性：例如 夏天时 冰淇淋的销量上涨 溺水人数也上涨 带来的pearson相关性高，这时两者其实并不存在实质性的关联关系，只是都和季节（夏天）相关。

| 指标                           | 衡量关系类型         | 适用场景        |
| ---------------------------- | -------------- | ----------- |
| **Pearson**                  | 线性             | 连续变量        |
| **Spearman**                 | 单调关系（非线性也行）    | 排序/非线性关系    |
| **Kendall**                  | 秩相关            | 样本较小时稳健     |
| **互信息 (Mutual Information)** | 任意关系（线性、非线性都可） | 分类、连续变量混合场景 |
| **Chi-square (卡方)**          | 离散型变量间关系       | 类别特征与标签     |

   - 重要性分析：一般采用模型（如Random Forest、XGBoost）自动评估特征贡献
     - 它可以解决一些非线性以及复合特征的相关性问题

| 相关性 | 重要性 | 建议           |
| --- | --- | ------------ |
| 高   | 高   | 核心特征         |
| 高   | 低   | 可能共线，可部分去除   |
| 低   | 高   | 非线性或交互特征，应保留 |
| 低   | 低   | 可剔除特征        |

**需要注意的点**  

| 关键点                 | 说明                            |
| ------------------- | ----------------------------- |
| **不要只看 Pearson**    | 它只捕捉线性关系，非线性关系用 Spearman 或互信息 |
| **保持物理逻辑判断**        | 有时候“高相关”但无物理意义 → 伪相关          |
| **注意共线性问题**         | 多个高相关特征会互相稀释重要性               |
| **特征重要性要用多个模型交叉验证** | 不同模型的排名略有差异，综合更稳健             |
| **可用 SHAP 值分析方向性**  | 能看出特征对目标的“正负影响趋势”             |

2. 创造新特征
   - 目的：提升模型表达能力
   - 常用方法包括：
     - 数学变换：log、平方、根号等
     - 组合特征：比例、差值、交互项（如 x1 * x2）
     - 聚合特征：基于时间窗口或分组计算平均值、方差等
     - 非线性映射：用 PCA、ICA、AutoEncoder 等生成新特征
   - 组合特征或延申特征
     - 例如： `温度` 和 `压力` 组合后对电容产生直接影响，可以生成新特征温压
     - 例如： 直方图显示房屋年龄有两个峰值：20-30 年（中年房）和 40-50 年（老年房），且这两个区间的房价分布差异明显（中年房均价高于老年房）。
       - 新特征：房屋年龄分组 定义：0 = 年轻房（<20 年），1 = 中年房（20-30 年），2 = 老年房（30-40 年），3 = 超老年房（>40 年）。
       - 作用：将连续的年龄变量转化为有业务意义的分组，捕捉 “不同年代房屋” 的价值差异（模型能明确学到 “中年房” 的溢价规律）。

3. 验证其效果
   - 目的：检验特征有效性
   - 方法：相关性检测、采用模型评估
  
<br><br>
## 三、 模型选择 ##
`回归类数据分析预测多采用线性模型和树模型`
> 模型选择多会选择多个模型训练并比较不同模型或不同超参数哪个更稳健。  

<br>

### 1. 线性模型 ###
数据遵从线性分布，但整体模型对数据限制较多  

### 2. 树模型 ###
`树模型是一类基于决策树结构的机器学习方法， 通过不断的条件分割将复杂问题简单化`

| 类型                            | 全称         | 特点                          |
| ----------------------------- | ---------- | --------------------------- |
| Decision Tree                 | 决策树        | 单棵树，结构清晰，可解释性强，但易过拟合。       |
| Random Forest                 | 随机森林       | 由多棵树组成的集成模型，用随机特征与样本提升泛化能力。 |
| Gradient Boosting Tree (GBDT) | 梯度提升树      | 多棵树串联，每棵树修正前一棵树的残差。         |
| XGBoost / LightGBM / CatBoost | 现代高性能 GBDT | 在工程优化与正则化上改进，广泛用于工业与竞赛。     |

`其中XGBoost是梯度提升树的工程化改进版本`

**树模型的结构（内部构成）**  
树模型是一棵由节点（nodes）和分支（branches）组成的有向树结构：

                 根节点 (root)
                /             \
            分支条件1          分支条件2
            /                     \
        子节点（叶节点）         子节点（叶节点）
  

> 根节点（Root Node）：包含所有训练样本。
> 分裂节点（Internal Node）：基于某个特征与阈值条件（如 x1 < 3.5）将数据分成两部分。
> 叶节点（Leaf Node）：停止分裂，输出一个预测结果（回归：均值；分类：多数类）。
> 每次分裂的目标是：
> - 让分裂后的两个子集“更纯净” —— 也就是让同类样本尽可能被分到一起。
> 纯净度衡量指标：
> - 分类问题：用 信息增益（Information Gain）、基尼系数（Gini impurity）。
> - 回归问题：用 平方误差（MSE）或方差下降。

<br>

**树模型的原理（训练过程）**  
递归划分（递归二叉分裂）
 - 算法从根节点开始：
   - 枚举每个特征（Feature）
   - 在该特征的所有可能分裂点上计算“纯度提升”
   - 选择最优的特征和阈值
   - 把数据分为两个子集
   - 对子集递归重复这个过程
   - 直到满足停止条件（例如样本太少、纯度提升太小、树太深）

<br>

剪枝（Pruning）
防止过拟合
可以是预剪枝（提前停止生长）或后剪枝（先长满再回头删除不必要分支）

<br>

预测阶段
- 对新样本，从根节点开始判断每个条件，沿路径一直走到叶节点
- 输出叶节点的预测值（均值或多数类）

<br>

**树模型的主要应用场景**  
树模型几乎可以用于所有结构化数据任务：
- 回归任务: 销售预测、用电量预测、设备寿命预测、电容数量预测等。
- 分类任务: 异常检测、客户流失预测、质量判定、设备故障分类。
- 特征重要性分析: 可以输出`特征重要度`，帮助你解释哪些特征最关键。
- 缺失值与类别型数据处理: 对缺失、类别、不平衡、非线性关系都具有天然鲁棒性。
<br>

**树模型对数据形式不敏感**  
这点是树模型的主要优势

| 方面        | 树模型的特性                   | 对应敏感模型          |
| --------- | ------------------------ | --------------- |
| **数值尺度**  | 不需要标准化 / 归一化。树只关心“大小关系”。 | 线性模型、SVM 对尺度敏感。 |
| **特征分布**  | 不要求正态分布。可直接处理偏态、离散特征。    | 线性模型常假设分布近似正态。  |
| **缺失值**   | 可自动分裂处理缺失样本（某些实现支持）。     | 线性模型需补全。        |
| **类别特征**  | 可直接使用分类分裂或频率编码。          | 多数模型需 one-hot。  |
| **非线性关系** | 通过多层分裂天然捕捉非线性。           | 线性模型无法直接拟合非线性。  |
| **异常值**   | 不易被极端值影响（分裂基于排序，不依赖均值）。  | 线性模型、均值统计敏感。    |

> 树模型基于“排序”和“区间划分”，不依赖线性假设、距离度量或分布形式，因此对数据的形式（单位、分布、取值范围）非常不敏感。

<br>

**树模型的优点**

| 优点                       | 说明                                 |
| ------------------------ | ---------------------------------- |
| 高解释性               | 结构是人能理解的 if-else 逻辑，可直接画树或计算特征重要性。 |
| 非线性建模能力强           | 多层划分能自动捕捉复杂非线性关系。                  |
| 无需数据预处理            | 不需归一化、标准化或线性假设。                    |
| 抗异常点能力强            | 基于排序划分，极端值不会影响整体分裂。                |
| 自动特征选择             | 分裂时会自动选最有用的特征。                     |
| 泛化能力强（尤其随机森林/GBDT） | 集成后稳定性和准确率都高。                      |
| 通用性强               | 可用于分类、回归、排序、特征筛选、重要度评估。            |

<br>

**树模型的缺点**

| 问题           | 后果             | 解决方案         |
| ------------ | -------------- | ------------ |
| 单棵树容易过拟合     | 对训练集贴得太紧       | 用随机森林或梯度提升树  |
| 不擅长外推        | 超出训练范围的数值预测不准确 | 与线性模型或神经网络结合 |
| 对少数类别偏倚      | 类别不平衡时分裂偏向多数类  | 加权采样、调整损失函数  |
| 不适合高维稀疏（如文本） | 分裂效率低          | 用线性/深度模型     |

<br>

**总结**

| 维度     | 决策树   | 随机森林           | GBDT / XGBoost / LightGBM |
| ------ | ----- | -------------- | ------------------------- |
| 模型结构   | 单棵树   | 多棵独立树（bagging） | 多棵串联树（boosting）           |
| 特征重要性  | 可解释   | 可解释（平均多棵）      | 可解释（基于增益）                 |
| 训练速度   | 快     | 中              | 较慢（但可并行）                  |
| 精度     | 中     | 高              | 更高                        |
| 是否过拟合  | 易     | 不易             | 可正则化控制                    |
| 是否需预处理 | 否     | 否              | 否                         |
| 典型应用   | 解释性场景 | 稳健预测           | 精确预测、竞赛、工业                |

## 四、 模型训练与评估 ##
1. 模型训练：采用交叉验证方式 区分训练集与测试集（也可以追加验证集），一般8-2划分或7-3划分。
   - 最常用的方式`K 折交叉验证（K-Fold Cross Validation）`
   > 核心思想：把全部样本平均分成 K 份（folds），轮流拿其中一份做验证，剩下的 K-1 份做训练。
2. 模型评估：
   - 常用指标：
     - MSE（Mean Squared Error）：平均平方误差，突出大误差（惩罚离群更严重）。
       - 相当于残差的方差，对于正态分布的结果来看，当MSE值大的话体现在正态分布结果整体被压扁拉长了
       - MSE 通过平方运算自动放大这些高风险的显著误差，让其在指标中占比更高，便于优先发现批次中的 “问题电容”。
       - MSE是精准聚焦那些可能影响您产品性能的 “显著偏差”，帮您判断电容批次的整体可靠性、生产稳定性，最终降低业务中的质量风险和成本损失。
       - 注意：
         - 对于数据值本身较小（如 0～1）的场景，MSE > 0.1 可能已经很大。（0.1相对真实值偏差了30%）
         - 对于数据值较大（如 0～1000），MSE 要看跟真实值的数据方差比是否大。
       - 公式： <img width="266" height="89" alt="image" src="https://github.com/user-attachments/assets/c3155f3a-f7b7-4399-beda-e0bf335efc17" />
       > $\hat{y}_i$： 是预测值  
       > $y_i$： 是实际值  
       > 𝑛 ： 是样本数量
       
       > 如果 MSE 远大于 MAE²：说明批次中存在少量大幅偏差的电容（如多数偏差 ±2μF，但个别偏差 ±10μF）。
       > 业务意义：可能存在生产异常（如设备故障导致的 “坏件”），需紧急筛查剔除。​
       > 如果 MSE ≈ MAE²，说明偏差普遍较小且均匀（如多数偏差在 ±3μF 内）。​​
       > 业务意义：整体质量稳定，风险主要来自系统性偏差（如工艺微调即可改善）。

     - RMSE（Root MSE）：均方根误差，单位与原始目标一致，常用。
       - 可以参考MSE 原理相同，只是对MSE开根号，数据更直观，用可理解的单位（如 μF）量化预测偏差
     - R²（决定系数）：解释方差比例：1 表示完美拟合，0 表示不比常数模型更好，负值表示很差。
       - R² 其实就是一个模型解释能力打分指标，满分是 1（或 100%），最低可能是负数（比乱猜还差）。模型到底能解释多少真实数据的变化趋势
       - 能告诉你模型到底有没有“抓住”趋势避免只看误差而忽略模型对趋势的把握（比如 MAPE 低但趋势错）
       - R²的原理：1 - （模型误差 / 基准误差）。
       - 比喻
         - 假设电容设计者，要预测每个型号的电容容量。
         - 如果 R² = 1.0 → 你的预测和实际完全一致，业务部门觉得你就是高手。
         - 如果 R² = 0.9 → 你的预测能解释 90% 的销量波动，只剩 10% 的变化没预测到（可能是突发事件）。
         - 如果 R² = 0.0 → 你的预测和直接用平均值没区别。如果 R² < 0 → 你的预测比直接猜平均值还差，业务方可能会怀疑模型有 bug。
     - MAE（Mean Absolute Error）：平均绝对误差，直观、对异常值不太敏感。
       - MAE 直接把每个预测误差取绝对值并平均，反映“平均上偏离多少单位”
       - MAE 的核心价值：直观、均衡地衡量预测偏差的平均水平，对极端偏差不敏感，适合需要整体稳定性或简化解释的场景。在电容预测中，它与 RMSE 互补：RMSE 警惕大偏差风险，MAE 反映平均精度水平，两者结合可更全面地评估预测模型的可靠性。
     - MAPE（Mean Absolute Percentage Error）：相对误差（注意对接近 0 的目标敏感）。
       - MAPE 把MAE除以真实值，变成比例，再取平均。这样就可以跨不同量级的数据做比较，也能判断预测的相对准确度。
       - MAPE 的核心价值：百分比表达，直观易懂、可跨产品、适合 KPI 考核： 很多预测 KPI 用百分比，比如“预测准确率需 ≥90%”（等价于 MAPE ≤10%）。
       - MAPE = 平均来说，我们的预测结果和真实值之间相差了多少 百分比。它跟 MAE 很像，只不过 MAE 是“差了多少个单位”，而 MAPE 是“差了多少百分比”，更容易做相对误差比较。
       - 缺点：当真实值接近 0 时，百分比会被放大甚至失真。

   <img width="1966" height="1011" alt="image" src="https://github.com/user-attachments/assets/d5396e68-19e8-4ce8-bc41-510fc8e2aaa3" />

   <img width="1347" height="1018" alt="image" src="https://github.com/user-attachments/assets/215c23c6-4cd9-4507-b601-07575a5a2f81" />



   - 如何选择：
     - 如果对大偏差惩罚严重（例如预测电力峰值），用 MSE/RMSE。
     - 如果你想要可解释的“平均偏差”，用 MAE。
     - 如果目标跨量级，考虑对数变换后计算 MSE（或使用 MAPE/SMAPE）。




