# 一、 树深度的调整 #
**控制单棵决策树的复杂度，深度越大，树能捕捉的特征交互越精细**  

1. 取值范围：3-10（默认 6），不建议超过 10  
2. 调优：从 3 开始逐步增大，直到验证集指标（如 RMSE）不再下降；若验证集指标上升，说明过拟合，停止增大  

> 深度越大 → 过拟合风险越高  

<br>

# 二、 叶子节点最小样本数 #
最小子代权重 → 叶子节点最小样本数  
**限制叶子节点的稀疏程度，数值越大，越难生成复杂树**    

1. 取值范围：1-20（默认 1）  
2. 调优：数据有极端值时增大（如 5-10），减少极端样本干扰；若模型拟合不足（训练集指标差），则减小  

> 数值越大 → 过拟合风险越低  

<br>

# 三、 正则化参数 #
**惩罚模型权重，抑制极端权重，降低过拟合风险**  

- L1（reg_alpha）：可让部分特征权重为 0，实现特征筛选  
- L2（reg_lambda）：让权重更平滑，核心防过拟合  

1. 取值范围：0-10（默认 1）  
2. 调优：模型过拟合时，优先增大reg_lambda；特征维度高时，搭配增大reg_alpha  

> 数值越大 → 正则化越强 → 过拟合风险越低  

<br>

# 四、 学习率 # 
Eta → 学习率（learning_rate）   
**控制每棵树对最终预测的贡献权重，步长越小，模型学习越平缓。**  

1. 取值范围：0.01-0.1（默认 0.1）  
2. 调优：遵循「小学习率 + 多迭代」黄金法则，如从 0.1 降至 0.05/0.01，需同步增加迭代次数  

> 步长越小 → 泛化能力越强 → 过拟合风险越低  

<br>

# 五、 迭代次数 # 
提升舍入次数 → 迭代次数  
**控制生成的决策树总数，数量越多，模型拟合越充分**  

1. 调优：搭配学习率调整，结合早停法（early_stopping_rounds），当验证集指标连续 N 轮（如 50 轮）不下降时，停止训练，取最优迭代次数  
2. 小学习率对应多迭代（如 lr=0.01 对应 n_estimators=1000）  

> 次数过多 → 过拟合风险越高；早停法可有效规避

