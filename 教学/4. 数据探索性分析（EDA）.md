
# 一、 探索性分析的目的 #
## 1. 查看数据质量 ##
- 保证数据可靠足以支撑模型预测
- 检测内容：
  - 缺失值
  - 重复值
  - 异常值
<br>

## 2. 理解数据特征和分布 ##
- 识别变量类型（连续型、离散型、类别型、有序型、时间序列等），为后续分析方法选择提供依据
- 查看各个特征的均值、中位数、众数、方差、标准差、偏度、峰度、唯一值数量、熵等  
- 可视化：  
  - 箱线图：查看数据的中心趋势、离散程度和异常值。  
  - 直方图：展示单个变量的数值分布情况——数据集中、偏态、异常  
  - Q-Q图：查看数据是否服从某种理论分布（通常是正态分布）。  
  - 散点图：探索两个连续变量之间的关系——线性/非线性、相关性、离群点  
<br>

## 3. 发现异常和噪声 ##
- 直观查看缺失、唯一、重复等数据状况  
- 通过可视化内容初步判断数据分布及异常、为数据清洗做准备  
<br>

## 4. 提取潜在意义 ##
- 通过相关性分析、字段业务含义、可视化洞察，提取有用信息，例如：  
  - 数据分布趋势  
  - 组合特征（如 A×B）  
  - 分箱特征（如将连续温度分为“低温/中温/高温”）  
  - 冗余特征识别（高度相关特征可合并）  
<br>

## 5. 为建模提供依据 ##
- EDA 结果指导数据清洗、特征工程、特征筛选、特征转换等建模前处理步骤  
<br>

## 6. 对齐业务目标与数据现实 ##
- 验证数据是否包含解决业务问题所需的关键信息  
- 识别数据盲区（如缺失关键工艺参数）  
- 评估预测目标的可建模性（如质量、数据多态）
<br>

# 二、 探索性分析的流程 #
1. 数据质量检查
2. 可视化分析
3. 特征相关性与共线性分析
4. 异常和噪声的提取
5. 潜在含义的提取及对齐业务

# 三、 如何进行探索性分析 #
以SPSS Modeler为例子  
数据参照上层目录的`data.csv`   
<br>

## 1. 数据质量 ##
<img width="2826" height="1287" alt="image" src="https://github.com/user-attachments/assets/e5d46fff-51e9-4ffa-b3cd-85da84c96d47" />
<img width="2824" height="1230" alt="image" src="https://github.com/user-attachments/assets/aa8aefe0-f6c9-47e9-9100-0f73bea8e896" />

<br>
<br>

**查看数据度量和数据类型**  
> 数据度量分为：连续、有序型、名义型、标记型、分类型    
> - 连续：数值可以连续变化，比如身高（160cm、160.5cm、161cm…）。    
> - 分类 / 名义：属于类别型但无顺序，比如地区（北京 / 上海 / 广州），多用于分组统计。    
> - 有序：类别有明确顺序，比如满意度（低 / 中 / 高），带有趋势的分类数据。  
> - 标志：通常是二分类（如是否、0/1）。  
> - 无类型：工具暂时无法或不需要定义其类型，基本不参与模型训练。  
>  
> 数据类型分为：字符串（文字型）、整数、浮点型（小数型）等  

<br>

**最小值/最大值**
> 最小值最大值反应了数据的分布范围，例如生产温度在0~100度之间。 数据不同的分布区间会产生不同数据走向从而影响的预测结果。  

<br>

**平均值**
> 结合最大值最小值等，可以反映数据分布是否均匀、是否存在偏态或异常，作为数据波动的一种观测手段。  

<br>

**中位数**
> 数据中50%的位置，和平均值相似，结合最大值最小值等，可以反映数据分布是否均匀、是否存在偏态或异常，作为数据波动的一种观测手段。多为体现是否有极值出现拉偏中位数。  

<br>

**众数**
> 数据中出现次数最多的数值或文字， 反映数据的集中分布情况。  

<br>

**分位数**
> Q1/Q3， 一般为25分位数和75分为数据，多作为箱线图的基础数据，反映数据主体分布情况以及极端值和界外值。  

<br>

**标准偏差**
> 一般与平均值做比较  
> 标准差反映的是数据距离平均值偏差了多少，原理是反映数据的波动情况。  
> 标准差是方差的开方

<br>

**方差**
> 与标准差类似，描述数据的离散程度（或数据波动）  
> 方差会放大极端值的影响。

<br>

**偏度**
> 反映数据是否偏向哪一边

<br>

**唯一**
> 反映数据中是否存在重复值，如果全是重复值，则说明数据没有任何波动，模型完全抓取不对规律。  
> 如果不存在重复值，需要分情况考虑，例如id类字段，没有实质性含义且不重复，作为特征的话模型可能会极端的拟合id的数据分布产生过拟合的情况，但是也存在连续型数值，按照业务规律分布而不重复的场景，模型可以抓住这一规律。

<br>

**峰度**
> 是否有厚尾，峰度大大概率存在厚尾的情况，多以正态分布图展示这一情况。尾厚说明极端值越多。  

<br>

**有效**
> 反映数据中可用值的数量，主要排除了缺失值和非法字段，例如数值中混入了文字，或NaN。  

<br>

**极值/界外值**
> - 极端值：数据中最大或最小的观测值，若符合业务实际（如大促销量），就是正常极端值。  
> - 界外值：明显偏离整体分布且无合理解释的点（如身高3米、负销售额），通常为错误或异常事件。  
> 一般采用IQR或z-sorce计算。

<br>

##  ##

<br>

### 通过以上内容判断数据质量 ###
主要判断数据的`完整性`、`有效性`、`准确性`、`一致性`、`唯一性`、`数据分布的稳定性`。   
> - 完整性: 数据有没有缺(关键字段或数据的缺失)  
> - 有效性: 数据是否合法(是否超出业务范畴)  
> - 准确性: 数据是否产生突变(相同条件下产生不同结果)  
> - 唯一性: 有没有重复数据  
> - 分布稳定性: 数据是否离散(体现在数据多变，例如设备老化，且有可能存在少量且突出的数据)

<br>

##  ##

## 2. 可视化 ##
目前以直方图、箱线图、Q-Q图、散点图为主要观测手段，其他图也可辅助使用。  

### 直方图 ###
主要用于观测数据分布状态。  

> **什么是偏态分布什么是正态分布**  
> 偏态分为右偏态、左偏态和正态，其中右偏态又称为正偏态。  
> 如何判度偏态？  
> 右偏态：通过直方图观测数据从多到少从高到底的分布叫做右偏态分布，主要体现在长尾在右侧（长尾是数据逐渐减少且一直向右侧下降）。  
> 左偏态：与右偏态相反，长尾在左侧，有少到多由低到高的上升趋势为左偏态。   
> 正态分布：像是左右偏态的组合，由高点像左右两侧以下降趋势均匀分布。  
<img width="1916" height="531" alt="image" src="https://github.com/user-attachments/assets/789cfa2a-2fb2-4ce5-bfcc-3403f9560962" />

<br>

<img width="1294" height="1066" alt="image" src="https://github.com/user-attachments/assets/1cf04332-7d50-467c-8d3b-3bbf90fc472e" />

> 以上图为例，数据呈右偏态（正偏态）， 对于偏态的处理方式多采用为`分箱、截断、取对数、平方根、box-cox(自动多次幂)、Yeo-Johnson(可以处理正负数的自动多次幂)`。  
> 从图中观测，数据存在三个峰值，且整体趋势为下降趋势，说明数据分布不均。  
> 可以首先考虑分箱操作将数据分成多分分布均匀的块。  
> 按图示，有三个峰，且最后呈下降趋势，因此考虑分成三或四个块，每个块代表一种分布趋势，这种操作叫做`分箱`。    
> 从尾部来看，数据分布范围大致在 0 - 26000 左右 考虑到业务场景中一个月中加班26000分钟存在的几率较小，可能是一个`异常`范围。  

<br>

### Q-Q图 ###
同样也用于观测数据分布状态，主要观测数据是否呈正态分布。  

> Q-Q图中存在着一条拟合线，当数据均匀分布在拟合线上或在其左右时说明数据呈正太分布。  

<br>

<img width="1662" height="1063" alt="image" src="https://github.com/user-attachments/assets/d919fc58-df62-4659-ac08-0eee11ecc25c" />

> 从上图来看，结合之前的直方图，大致按红线将数据分为四种分布趋势块。基本每块切分后都呈正态分布。  
> 其中最后一块中的紫色点为离群值，映照了直方图中分析的异常部分。  

<br>

### 箱线图 ###
也用作观测数据分布，但主要为查看异常或极端数据。  

> 箱线图主要由箱体和左右长尾，以及离群点构成  
> 主要将数据分为25% 75%，这两条分割线被称为上四分位和下四分位，长尾值是算出的极限数据(1.5倍IQR 为长尾界限，3倍IQR 为极值界限)  
> IQR = 上四分位（Q3） - 下四分位（Q1）  

<br>

<img width="1573" height="1020" alt="image" src="https://github.com/user-attachments/assets/4c942bd2-62e6-453b-b1e1-63af556630ae" />

> 从上图可以看出，三角形离群点属于一个极端异常值，贴合之前直方图的分析。  
> 同时可以看出数据集中分布在1000 ~ 7000左右，中间值在4000左右。  
> 异常界限为0~15000  

<br>

### 散点图 ###
主要用于分析两个特征间的数据分布趋势。  

> 如果数据呈线性分布，则说明两者存在线性关系，例如：向上趋势说明随着特征a的增长b也在增长。  
> **但是两者之间的关系并不代表一定是散点图中表示的，也可能存在两个无关数据的偶发现象或其他特征的间接影响**  

<img width="1680" height="1027" alt="image" src="https://github.com/user-attachments/assets/0441da59-d548-41cb-82a5-883c52b96280" />

> 从上图来看数据呈下降趋势，但是分布并不均匀，主要集中在0-5000的区间。因此数据是非线性且为负提升关系。  
> 并且可以看出actual_productivity的上限是1.2下限是0。  
> 而且右侧的点比较离散，存在长尾现象，因此数据可能存在异常值或极端值的情况，这贴合了直方图的分析。  
> 因此结论为`树模型`相比`线性模型`要更能拟合这份数据。  

<br>

## 3. 初步探索新特征 ##   
通过业务结合特征，分析出可能存在的新特征，例如通过`加班时长/总时长`计算加班比率。  
通过数据转换或分箱操作，形成新特征。  
通过数据的取值范围考虑数据是否需要做归一化或标准化操作。  
通过数据类型将分类数据中的文字转换成数值。  











